# Most common optimizer is Gradient Descent - it's one of the most important methods in machine learning and one of the most basic ones.  In tensorflow you can search through all the optimizers, there are 7 types of optimizers for you to use.  Gradient descent is highly dependent upon how much data you pass to the training.  Example:  when you pass 1/10 or 1/100 of the data size into training Gradient Descent becomes Stociastic Gradient Descent.  Stociastic Gradient Descent only trains partial data for each training step, this can be an advantage as we can accelerate the learning time, it learns to go to the global or local minimum faster then pure Gradient Descent.  However the Gradient Descent spends more time in training becuase it minimizes all data samples every time.
# The next most common used optimizers are called Momentum Optimizers and Adam Optimizer.  AlphaGo and Atari use RMS Prop Optimizer to train
# All these optimizers only change the learning rate for each step.  For example, the momentum optimizer consider not only the learning rate for this training step but also considers the learning rate on the last step, or last few steps.  A lot of the others consider the learning rate similar to the momentum method, so they can also speed up your training time and get to the global or local minimum faster.  Stociastic Gradient Desecent like Gradient Descent is slower then the others.  While momentum considers the last learning rate, it will become faster as it runs, even if it starts slow or in the wrong direction it corrects this later on and speeds up
# As a beginner, it's a good idea to use Gradeient Descent and then start to experiment with the others, momentum and adam or RMS prop.  
