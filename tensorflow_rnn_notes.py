
# The popular cell in Recurrent Neural Net (RNN) is called the LSTM (Long-short term memory).  As long as you have a sequence of data, you can use RNN to predict them.  For example, you can remember your phone number by calling it from beginning to the end.  And for this the sequence in the phone number is very important for you to remember the number.  RNN is the best tool to deal with sequence of data.  The RNN is basically the same idea as the CNN but in time space.  For example, in this RNN, x1 -> w1 -> y1, x2 -> w2 -> y2, x3 -> w3 -> y3, x4 -> w4 -> y4, you can treat w1 as a single network and the same for the others(w2, w3, w4). The x is the input and the y is the output.  If you want to predict the sequence of Y based on a sequence of X, it's better to have a filter like what CNN has, use the shared weights as memory, so all of this can be concluded as one weight or Cell.  Using the cell to calculate x1 and predict y1, using the same cell to calculate y2 based on x2 and the memory of x1, so it's like this, I have old memory x1 and new memory x2 and I combine them to get y2.  So y2 is not only based on x2 but also x1.  So the w are the same w like filter in the CNN, w will keep the memory of x1, x2, x3...  So just like x1 = A, x2 = B, x3 = C, to predict the next value x4, it will be based on ABC, instead of just the last C.  So the sequence itself is your memory, the next character is the one after the whole sequence.  The next output is the new memory + the old sequence memory.  The State is the record of previous sequences.  To get S(t) which is state(time), you consider S(t-1) with the x(t).  This the recurrent part, the relationships between x, s, W, o -> inputs, state, weights, output.  Now onto the LSTM, this is a very powerful structure.  The original RNN may have some problems with the vanishing graident and gradient exploding, if your states are very long the backpropogated gradients may not be able to pass to the beginning or even explode when you pass to the beginning.  If the gradient value is less than 1, it might vanish.  For example, the value is 0.9, when you pass it through one state, it will pass 0.9 to the next and 0.9 to the next and so on, so think about 0.9 ^ n, eventually it would just be 0 so there will be no gradient at the beginning. If it's greater then 1.1 ^ n it will explode.  The LSTM is designed to solve the vanishing gradient problem, it will add three gates (write, read, forget) into RNN cell.  The gate will include a trainable matrix, it can be trained as well.  The write gate controls: should I remember the input?  The read gate controls: should I read the results?  The forget gate controls: should I forget the previous memory.  With these 3 gates, we can limit the vanishing graident.  The memory in RNN is just like the main storyline when you watch a TV series.  The LSTM provides a sub storyline, this is the same as a computer game.  The main storyline is constructed by the sub storyline.  Now we have a main storyline with a new event.  The write gate tells us whether we should write the sub storyline or the new event into the main storyline.  Another example, you have a detective story, the main storyline tells us suspects A and B are the most likely culprits.  And now a sub storyline tells us C may be another suspect.  The write gate can judge the C and tell us should I consider C as the suspect that can affect the main story line.  If the C should not be considered the write gate will just ignore this event.  The forget gate, if the sub storyline is so important, I will forget what happened temporarily and only consider now suspect C is important and choose to forget A and B temporarily and only consider C.  The read gate indicates whether I should tell you the story now.  These 3 gates are the main structures in the LSTM.
